{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "source": [
        "#Conversational Chatbot\n",
        "\n",
        "\n",
        "## Syed Mehedi Hasan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Fkx7nqdWrY",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xckxs7U9ML4R",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "source": [
        "## Readme\n",
        "###How to Run:\n",
        "\n",
        "\n",
        "#### 1. To start chatting please goto Menu ->Runtime-> Run all or directly press (ctrl + F9) from keyboard.\n",
        "#### 2. Please click on google link on cell 1.1.2 and provide google log in detail, it will give authentication ID please copy authentication code and paste in input box below the link\n",
        "\n",
        "#### 3. Then please go to cell 3.5.2. Execute program - chatting mode (please type \"xxx\" to Exit from current personality mode and enter 0 to exit chatbot)\n",
        "####All Models, ipython notebook and chatlogs are available in the link below: https://drive.google.com/open?id=1HDThzAsIsDxADWR5XEIV9DxZdDEZUr4n\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jnbsPlUcTTm",
        "colab_type": "text"
      },
      "source": [
        "###1.1.1 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GOTCJwg-pt4",
        "colab_type": "code",
        "outputId": "8669d1aa-6c95-4ca7-83d4-cd9f3598537f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#1.1----Load---libraries------\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pprint\n",
        "import re\n",
        "from lxml import etree\n",
        "import nltk\n",
        "\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "from gensim import models\n",
        "import datetime\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKsrODh3ccSF",
        "colab_type": "text"
      },
      "source": [
        "###1.1.2 Download Personality chatbot data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "outputId": "0e6c9962-ad02-472e-dfa2-414fbb9c4c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Install PyDrive and import libraries \n",
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "\n",
        "id = '1T6b7WLUeAchBKnaP10gH2aSOEARJLM6Q'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv') \n",
        "downloaded = drive.CreateFile({'id':'1Jl-m5Q_SRDF_VfA8QT1lrE-1k2Vc6pYG'})\n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv') \n",
        "downloaded = drive.CreateFile({'id':'1Dlm69VvrxC4Y8NS2abK3B5jKtos8sy2C'})\n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv') \n",
        "\n",
        "df_prof = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
        "df_fr = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "df_com = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.8MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.7MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.4MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.0MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.7MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.3MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 4.9MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 5.0MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 6.8MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 6.7MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 12.3MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 12.5MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 12.5MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 12.7MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 12.8MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 12.8MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 43.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 14.6MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 14.7MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 15.0MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 14.8MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 14.8MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 14.2MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 14.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 14.4MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 14.3MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 15.1MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 50.9MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 50.1MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 50.6MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 46.9MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 47.3MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 54.8MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 54.4MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 54.7MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 18.5MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 18.0MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 18.0MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 18.0MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 18.1MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 18.3MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 18.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 18.3MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 18.3MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 18.3MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 52.7MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 53.3MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 53.4MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 54.7MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 54.6MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 61.3MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 62.3MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 61.0MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 60.7MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 60.0MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 59.4MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 63.1MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 62.4MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 62.5MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 61.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 60.5MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 46.4MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 24.4MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 24.0MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 23.9MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 23.8MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 24.1MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 24.1MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 24.0MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 24.0MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 23.9MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 27.2MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 58.1MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 59.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 61.0MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 61.4MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 59.8MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 59.8MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 59.3MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 59.9MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 53.3MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 52.8MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 53.8MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 55.0MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 55.0MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 56.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 56.4MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 57.0MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 57.8MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 57.6MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 66.3MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 67.5MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 65.6MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.9MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdKI8E2KRwe",
        "colab_type": "text"
      },
      "source": [
        "####We used tokenization, remove unnecessary numbers and punctuations.Then we are converting all tokens to lowercase so that we can get same vector for each token word whether it is in uppercase, lower case or mixed case.We did not remove stop words as some questions are formed with only NLTK.corpus stop words. We keep three personality type data in three data frame and make the dictionary for three set answers. We added four special char (_E_, _B_, _P_,_U_) to each dictionary as it is very important.We used that to form input_batch, output_batch, and target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "outputId": "b0a41e39-986b-4b36-bc6a-742ea7972a42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "# Read Personality chat datasets\n",
        "df_prof.head(4)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>Answer</th>\n",
              "      <th>Source</th>\n",
              "      <th>Metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What's your age?</td>\n",
              "      <td>Age doesn't really apply to me.</td>\n",
              "      <td>qna_chitchat_the_professional</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Are you young?</td>\n",
              "      <td>Age doesn't really apply to me.</td>\n",
              "      <td>qna_chitchat_the_professional</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>When were you born?</td>\n",
              "      <td>Age doesn't really apply to me.</td>\n",
              "      <td>qna_chitchat_the_professional</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What age are you?</td>\n",
              "      <td>Age doesn't really apply to me.</td>\n",
              "      <td>qna_chitchat_the_professional</td>\n",
              "      <td>editorial:chitchat</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Question                           Answer  \\\n",
              "0     What's your age?  Age doesn't really apply to me.   \n",
              "1       Are you young?  Age doesn't really apply to me.   \n",
              "2  When were you born?  Age doesn't really apply to me.   \n",
              "3    What age are you?  Age doesn't really apply to me.   \n",
              "\n",
              "                          Source            Metadata  \n",
              "0  qna_chitchat_the_professional  editorial:chitchat  \n",
              "1  qna_chitchat_the_professional  editorial:chitchat  \n",
              "2  qna_chitchat_the_professional  editorial:chitchat  \n",
              "3  qna_chitchat_the_professional  editorial:chitchat  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU7uFc5DcNz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#concat all three chat data frame together. we will add all questions from here to our movie line data\n",
        "# to make richer Word2Vec Model\n",
        "df_all=pd.concat([df_prof,df_fr,df_com])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDmdl0Xfgoxh",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.1 Tokenisation, stopwords removal, stemming etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzVJJiclfy3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Functions to pre-process data for Seq2Seq Model---\n",
        "\n",
        "#Maximum number of words in a question for a given Personality\n",
        "global max_input_words_amount\n",
        "#Maximum number of sentences will come as output\n",
        "global max_output_words_amount\n",
        "#Maximum number of unique answers we kept in dictionary\n",
        "global max_dic_len\n",
        "\n",
        "#Take the maximum number of unique answers in any chat data set\n",
        "max_unique_answers = max (len(list(set(df_prof['Answer']))), len(list(set(df_fr['Answer']))), len(list(set(df_com['Answer']))))\n",
        "\n",
        "#we added four specail chars to our unique answer\n",
        "max_dic_len=max_unique_answers + 4\n",
        "\n",
        "# Generate Sequence-Data from Question and answer data\n",
        "def get_seq_data(df):\n",
        "        \n",
        "        seq_data = []\n",
        "        whole_words = []\n",
        "                        \n",
        "        max_input_words_amount=0\n",
        "        max_output_words_amount=1\n",
        "               \n",
        "        question={}\n",
        "        answer={}\n",
        "\n",
        "        #Iterate over all rows of chat data frame\n",
        "        for index, row in df.iterrows():\n",
        "\n",
        "            ###\n",
        "            question[row[0]] = index\n",
        "            answer[row[1]] = index\n",
        "\n",
        "            tmp_qa=[]\n",
        "            tmp_qa.append(row[0])\n",
        "            tmp_qa.append(row[1])\n",
        "            seq_data.append(tmp_qa)\n",
        "           \n",
        "            \n",
        "            # tokenise question\n",
        "            #tokens_q=word_tokenize(row[0])\n",
        "            tokenized_q =word_tokenize(row[0].lower())\n",
        "\n",
        "            # we do not need to tokenise answer (because we implement N to One model)\n",
        "            # make a list with only one element (whole sentence)\n",
        "            #tokenized_a += row[1]\n",
        "            tokenized_a=row[1]\n",
        "\n",
        "            \n",
        "            # answer list (one element) as we keep only answer to dict\n",
        "            whole_words.append(tokenized_a)\n",
        "\n",
        "            # we need to decide the maximum size of input word tokens\n",
        "            max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
        "\n",
        "\n",
        "        # we now have a vacabulary list\n",
        "        unique_words = list(set(whole_words))\n",
        "\n",
        "        # adding special tokens in the vocabulary list    \n",
        "        # _B_: Beginning of Sequence\n",
        "        # _E_: Ending of Sequence\n",
        "        # _P_: Padding of Sequence - for different size input\n",
        "        # _U_: Unknown element of Sequence - for different size input\n",
        "\n",
        "        unique_words.append('_B_')\n",
        "        unique_words.append('_E_')\n",
        "        unique_words.append('_P_')\n",
        "        unique_words.append('_U_')\n",
        "        \n",
        "        #make all persnality answer set same length by adding _U_ + str(i+1), i=1 to length different\n",
        "        if len(unique_words)<max_dic_len:\n",
        "            diff=max_dic_len-len(unique_words)\n",
        "            for i in range(diff):\n",
        "                unique_words.append('_U_' + str(i+1))\n",
        "        \n",
        "        \n",
        "        unique_words.sort()\n",
        "        num_dic = {n: i for i, n in enumerate(unique_words)}\n",
        "        \n",
        "       \n",
        "          \n",
        "        dic_len = len(num_dic)\n",
        "        \n",
        "        return seq_data, num_dic, unique_words,dic_len\n",
        "      \n",
        " # get token index vector of questions and add paddings if the word is shorter than \n",
        " #the maximum number of words\n",
        "def get_vectors_q(sentence):\n",
        "    \n",
        "    # tokenise the sentence\n",
        "    \n",
        "    tokenized_sentence = word_tokenize(sentence.lower())\n",
        "    \n",
        "    #calculate lenth different between number of words in  this question and max_input_words_amount\n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    \n",
        "    # add paddings if the word is shorter than the maximum number of words    \n",
        "    for x in range(diff):\n",
        "        \n",
        "        tokenized_sentence.append(\"_P_\")\n",
        "  \n",
        "   \n",
        "    data = [num_dic[n] for n in tokenized_sentence]\n",
        "           \n",
        "    \n",
        "        \n",
        "    return data\n",
        "\n",
        "# get token index vector of answer\n",
        "def get_vectors_a(sentence, num_dic):    \n",
        "    tokenized_sentence = [sentence]\n",
        "    data = tokens_to_ids(tokenized_sentence, num_dic)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "# convert tokens to index\n",
        "def tokens_to_ids(tokenized_sentence, num_dic):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        \n",
        "        if token in num_dic:\n",
        "            ids.append(num_dic[token])\n",
        "        else:\n",
        "            ids.append(num_dic['_U_'])\n",
        "          \n",
        "\n",
        "    return ids\n",
        "  \n",
        "#Get Word2Vec for each word of question(sentence) and join those to an array of vector\n",
        "def get_w2vec(model,sentence):\n",
        "    \n",
        "    # tokenise the sentence and take lower case\n",
        "    \n",
        "    tokenized_sentence = word_tokenize(sentence.lower())\n",
        "      \n",
        "    \n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    \n",
        "    # add paddings if the word is shorter than the maximum number of words    \n",
        "    for x in range(diff):\n",
        "        \n",
        "        tokenized_sentence.append(\"_P_\")\n",
        "        \n",
        "                \n",
        "    data=[] \n",
        "    \n",
        "    #data = [get_vector(model, n) for n in tokenized_sentence]\n",
        "    #data = np.eye(dic_len)[data]\n",
        "    \n",
        "    #get vector for each word of a question from word2vec model\n",
        "    for w in tokenized_sentence:\n",
        "      data.append(get_vector(model, w))\n",
        "    \n",
        "        \n",
        "    return data\n",
        "  \n",
        "#get a vector corresponding to a word from given model\n",
        "def get_vector(model, word):\n",
        "  \n",
        "  \n",
        "  if word in model.wv:\n",
        "    vector = model.wv[word]  \n",
        "  else:\n",
        "    \n",
        "    vector = model.wv['_U_'] \n",
        "  return vector\n",
        "  \n",
        "  \n",
        "  # generate a batch data for training/testing\n",
        "def make_batch_new(seq_data, num_dic, model):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "    #target=[]\n",
        "    \n",
        "    dic_len=len(num_dic)\n",
        "    \n",
        "    for seq in seq_data:        \n",
        "        # Input for encoder cell, convert question to vector\n",
        "        input_data = get_w2vec(model,seq[0]) #get_vectors_q(seq[0])\n",
        "             \n",
        "        \n",
        "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        " \n",
        "        tmp_seq= seq[1]\n",
        "        output_data = [num_dic['_B_']]\n",
        "         \n",
        "        output_data += get_vectors_a(seq[1], num_dic)\n",
        "        \n",
        "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
        "       \n",
        "        target = [num_dic[seq[1]]]\n",
        "        tmp_target = num_dic['_E_']\n",
        "        target.append(tmp_target)                        \n",
        "        \n",
        "        \n",
        "       \n",
        "        #append word2vec vector for each word of question to input_batch\n",
        "        input_batch.append(input_data)\n",
        "        \n",
        "        # Convert number got for each answer from our dictionary to one-hot encode data\n",
        "        output_batch.append(np.eye(dic_len)[output_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "        #s=seq\n",
        "               \n",
        "\n",
        "    return input_batch, output_batch, target_batch\n",
        "\n",
        "#Set  global variables max_input_words_amount and max_output_words_amount\n",
        "def set_max_input_words_amount(seq_data):\n",
        "    \n",
        "    global max_input_words_amount\n",
        "    global max_output_words_amount\n",
        "    \n",
        "    max_input_words_amount=0\n",
        "    max_output_words_amount=1\n",
        "    for row in seq_data:\n",
        "  \n",
        "        tokenized_q =word_tokenize(row[0].lower())\n",
        "            \n",
        "        \n",
        "        # we need to decide the maximum size of input word tokens\n",
        "        max_input_words_amount = max(len(tokenized_q), max_input_words_amount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjE10QaNk1qc",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2KIG0vChSIt",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNAo0xx5m_PN",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.2 Pickle Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFEYc9az1Gz1",
        "colab_type": "code",
        "outputId": "04716843-6c88-4f76-8569-7187e8049e05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Comment this code as we used the function below to make those data: (seq_data, num_dic, unique_words)\n",
        "#and upload those data as pickled file in google drive and used those file to get data for model\n",
        "\n",
        "'''\n",
        "# make and upload these (seq_data, num_dic, unique_words) to google drive using google drive mount and torch library\n",
        "def upload_pickle_data(df, personality_extention):\n",
        "  \n",
        "    seq_data, num_dic, unique_words,dic_len =get_seq_data(df)\n",
        "\n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "    #Save model to google drive. Comment this part as already uploaded\n",
        "    dic_save_name = 'num_dic_' + str(personality_extention)\n",
        "\n",
        "    path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Pickle_data/{dic_save_name}\" \n",
        "    torch.save(num_dic, path)\n",
        "\n",
        "    seq_data_save_name='seq_data_' + str(personality_extention)\n",
        "    path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Pickle_data/{seq_data_save_name}\" \n",
        "    torch.save(seq_data, path)\n",
        "\n",
        "    unique_words_save_name='unique_words_' + str(personality_extention)\n",
        "    unique_words_path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Pickle_data/{unique_words_save_name}\" \n",
        "    torch.save(unique_words, unique_words_path)\n",
        "    \n",
        "    '''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# make and upload these (seq_data, num_dic, unique_words) to google drive using google drive mount and torch library\\ndef upload_pickle_data(df, personality_extention):\\n  \\n    seq_data, num_dic, unique_words,dic_len =get_seq_data(df)\\n\\n    drive.mount(\\'/content/gdrive\\')\\n\\n    #Save model to google drive. Comment this part as already uploaded\\n    dic_save_name = \\'num_dic_\\' + str(personality_extention)\\n\\n    path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Pickle_data/{dic_save_name}\" \\n    torch.save(num_dic, path)\\n\\n    seq_data_save_name=\\'seq_data_\\' + str(personality_extention)\\n    path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Pickle_data/{seq_data_save_name}\" \\n    torch.save(seq_data, path)\\n\\n    unique_words_save_name=\\'unique_words_\\' + str(personality_extention)\\n    unique_words_path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Pickle_data/{unique_words_save_name}\" \\n    torch.save(unique_words, unique_words_path)\\n    \\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dcuOryE2GWm",
        "colab_type": "text"
      },
      "source": [
        "#### Upload Pickled data for num_dic, Seq_data and unique_words to google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rhk4oAn545_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#comment this code as we already uploaded those data to google drive and will train model \n",
        "#after downloading that\n",
        "\n",
        "#upload_pickle_data(df_prof, 'prof')\n",
        "#upload_pickle_data(df_com, 'com')\n",
        "#upload_pickle_data(df_fr, 'fr')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ARpTyQzpVoF",
        "colab_type": "text"
      },
      "source": [
        "### Load Dictionary and Seq_Data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqoFv0-Iozd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1.2.2.1 --- Custom functions-- Load dictionary and list data for Seq-Data from Google drive--\n",
        "\n",
        "#load pickled data from google drive (num_dict, seq-data, ma) using google py drive and torch library\n",
        "\n",
        "def load_pickle_data(personality_type):\n",
        "      \n",
        "      #if profesional personality then download corresponding pickle data from google drive\n",
        "      if personality_type==1:\n",
        "        extension='prof'\n",
        "        unique_words_id='1-DXWJnYIw3IvdoM753kaFuV5B1_k29-c'\n",
        "        seq_data_id='1-DVdb6CAbvkg0LbCbJVempQOZqSeHo3Q'\n",
        "        num_dic_id='1-B5GfNBOdkLOru-AOziWgkK47diurGY4'\n",
        "        \n",
        "      #if comic personality then download corresponding pickle data from google drive\n",
        "      elif personality_type==2:\n",
        "        extension='com'\n",
        "        unique_words_id='1-M0qD6e_FmmnM1w7SAgkN07Zz5_q60Wo'\n",
        "        seq_data_id='1-KgsKQz3S9va2OY3QYi-tu0EygIAh8zU'\n",
        "        num_dic_id='1-KDAiqFcn_ZHkGfOJKq8eyijoay6P7zu'\n",
        "      \n",
        "      #if friend personality then download corresponding pickle data from google drive\n",
        "      elif personality_type==3:\n",
        "        extension='fr'\n",
        "        unique_words_id='1-Q6I46Glmmdsg6CgrLR8YjgXRsNvUkrp'\n",
        "        seq_data_id='1-O8QsFwaEgbVba25DhbaDo5ch4LG8wvJ'\n",
        "        num_dic_id='1-R_A0WLphGMmTpgXRKzlp3E_OBG2eiZq'\n",
        "\n",
        "             \n",
        "      file_unique_words = drive.CreateFile({'id':unique_words_id})\n",
        "  \n",
        "      file_unique_words.GetContentFile('unique_words_prof')\n",
        "      unique_words=torch.load('unique_words_prof')\n",
        "      \n",
        "      \n",
        "      \n",
        "      file_seq_data = drive.CreateFile({'id':seq_data_id})\n",
        "  \n",
        "      file_seq_data.GetContentFile('seq_data_prof')\n",
        "      seq_data=torch.load('seq_data_prof')\n",
        "      \n",
        "      \n",
        "            \n",
        "      file_num_dic = drive.CreateFile({'id':num_dic_id})\n",
        "  \n",
        "      file_num_dic.GetContentFile('num_dic_prof')\n",
        "      num_dic=torch.load('num_dic_prof')\n",
        "      \n",
        "      #Set maximum number of words in any single question of this personality chat.\n",
        "      set_max_input_words_amount(seq_data)\n",
        "      return seq_data, num_dic, unique_words, len(num_dic)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzVJJxaisApE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load seq_data, num_dic for professional type chat as default type from pickled file in google drive\n",
        "seq_data, num_dic, unique_words,dic_len =load_pickle_data(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KkkxB8uXL6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzm-NWBTmM-",
        "colab_type": "text"
      },
      "source": [
        "#####We used Word2Vec with CBOW\n",
        "#### We used size of vector size as length of dict_len i.e total number of unique answers. It is very important otherwise our shapes of our input-batch and output-batch  will be different, that will give trouble to Neural Network.We here keep dic_len same for all personality to use NN in same format.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cM4rlYkHefJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Please comment your code\n",
        "#Generate a cleaned data for Word2Vec \n",
        "\n",
        "def get_cleaned_data(df):\n",
        "  \n",
        "   \n",
        "    words=[]\n",
        "    clean_words=[]\n",
        "    \n",
        "    for index, row in df.iterrows():\n",
        "      \n",
        "            sents_lst=[]\n",
        "            sent1=row[0]\n",
        "           \n",
        "            sents_lst.append(sent1)\n",
        "                        \n",
        "            for sent in sents_lst:\n",
        "                  #apply regular expresion to whole text to remove number and punctualtion\n",
        "                  sents1 = re.sub(r\"[^a-z0-9]+\", \" \", sent.lower())\n",
        "                  #tokenize as\n",
        "                  sent_text=sent_tokenize(sents1)\n",
        "\n",
        "                  normalized_text = []\n",
        "\n",
        "                  for string in sent_text:\n",
        "                      #apply regular expresiont to each sentence again\n",
        "                      tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "                      normalized_text.append(tokens)\n",
        "\n",
        "                      # Tokenising each sentence to process individual word\n",
        "                      sentences=[]\n",
        "                      sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "                      \n",
        "                      #remove stop words using NLTK stop word dictionary for English Language\n",
        "                      sentences=remove_stop_words(sentences)\n",
        "                      clean_words.append(sentences)\n",
        "           \n",
        "      \n",
        "    #Flattend the list, reduce to one dimension lower\n",
        "    flattened = [val for sublist in clean_words for val in sublist]\n",
        "    \n",
        "    clean_words=flattened\n",
        "    \n",
        "    sents=[]\n",
        "    sents.append('_B_')\n",
        "    sents.append('_E_')\n",
        "    sents.append('_P_')\n",
        "    sents.append('_U_')\n",
        "    clean_words.append(sents)\n",
        "    return clean_words\n",
        "\n",
        "# Function for Removing Stop Words using NLTK stop word dictionary for English language\n",
        "\n",
        "def remove_stop_words(all_words):\n",
        "  \n",
        "    all_words_new=[]\n",
        "    for i in range(len(all_words)): \n",
        "        \n",
        "        sent_words=all_words[i]\n",
        "        sent_words_new=[]\n",
        "        for word in sent_words:\n",
        "            if word not in stopwords.words('english'):\n",
        "                sent_words_new.append(word)\n",
        "        all_words_new.append(sent_words_new)\n",
        "        \n",
        "    return all_words_new\n",
        "  \n",
        "\n",
        "#Generate a a single of Word2Vec Model\n",
        "def generate_word2vec_model(df):\n",
        " \n",
        "    #Get cleaned and processed data\n",
        "    lst_words=  get_cleaned_data(df)\n",
        "  \n",
        "    tmp_model=Word2Vec(sentences=lst_words, size=max_dic_len, window=5, min_count=1, workers=4, sg=0)\n",
        "    tmp_model.train(lst_words, total_examples=len(lst_words), epochs=10)\n",
        "    \n",
        "    \n",
        "    return tmp_model\n",
        "\n",
        "\n",
        "#get a vector corresponding to a word from given model\n",
        "def get_vector(model, word):\n",
        "  \n",
        "  \n",
        "  if word in model.wv:\n",
        "    vector = model.wv[word]  \n",
        "  else:\n",
        "    \n",
        "    vector = model.wv['_U_']\n",
        "    \n",
        "  return vector\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny9an-55YE_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Op66omXKVHa",
        "colab_type": "text"
      },
      "source": [
        "#### To build a rich Word2Vec model we chose to use Corneille Movie data. The idea behind that is if we train our Word2Vec on large amounts of natural language data, we should be able to get  vector representations for each word that capture the semantic similarity between words. So, we can a get much reliable vector corresponding to each word of user question. We also added all questions from Microsoft chat data to get richer Word2Vec model. So, our word2vec mdel is richer and specially this microsoft chat data, they will be able to give more acurate vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrI-q0N4AgWH",
        "colab_type": "code",
        "outputId": "a6f08d72-89ec-426f-9443-084ae3470c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "#Read movie_lines,txt file and clean and process data, store in pandas data frame\n",
        "\n",
        "\n",
        "file_movie = drive.CreateFile({'id':'1ZvYX0VnPqd9nU2oJ9x6BwsqvGnXpQjgG'})\n",
        "file_movie.GetContentFile('movie_lines.txt') \n",
        "\n",
        "with open(file_movie['title'],encoding='utf-8', errors='ignore') as f: \n",
        "    lines = f.readlines() \n",
        "\n",
        "sents=[]\n",
        "for line in lines:\n",
        "        words = line.split('+++$+++')\n",
        "        str1=words[4]\n",
        "        i=str1.index('\\n')\n",
        "        str2=str1[:i]\n",
        "        sents.append(str2)\n",
        "\n",
        "# adding special tokens in the vocabulary list    \n",
        "# _B_: Beginning of Sequence\n",
        "# _E_: Ending of Sequence\n",
        "# _P_: Padding of Sequence - for different size input\n",
        "# _U_: Unknown element of Sequence - for different size input\n",
        "\n",
        "sents.append('_B_')\n",
        "sents.append('_E_')\n",
        "sents.append('_P_')\n",
        "sents.append('_U_')\n",
        "\n",
        "df_movie=pd.DataFrame(sents, columns=['Question'])\n",
        "df_movie.head(3)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>They do not!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>They do to!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I hope so.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Question\n",
              "0   They do not!\n",
              "1    They do to!\n",
              "2     I hope so."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "source": [
        "####We used tokenization, remove unnecessary numbers and punctuations.Then we are converting all tokens to lowercase that we can get same vector for each token word whether it is uppercase, lower case or mixed case.Finally we removed stops using NLTK stopwords dictionary. \n",
        "We added the following chars to our word2vec model\n",
        "._B_: Beginning of Sequence\n",
        " _E_: Ending of Sequence\n",
        " _P_: Padding of Sequence - for different size input\n",
        " _U_: Unknown element of Sequence - \n",
        " The reason is we added those with our input-batch and output batch  when we send user question to model in chat mode\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "outputId": "efdb2516-6a19-42c1-922c-a56f74c1f022",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Add all Questions of all personality from Microsoft chat data to movie data frame\n",
        "#We comment this as we already build Word Embedding Model and uploaded to google drive and\n",
        "#Use that model after downloading that\n",
        "\n",
        "'''\n",
        "df_tmp=df_movie\n",
        "\n",
        "for i, row in df_all.iterrows():\n",
        "    df_tmp = df_tmp.append([{'Question':row[0]}], ignore_index=True)\n",
        "\n",
        "df_movie=df_tmp\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ndf_tmp=df_movie\\n\\nfor i, row in df_all.iterrows():\\n    df_tmp = df_tmp.append([{'Question':row[0]}], ignore_index=True)\\n\\ndf_movie=df_tmp\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NPVXzX2nxFw",
        "colab_type": "text"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.3. Build and Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "source": [
        "#### We have used here Gensim library function to build and train model. Only few parameters we set here that we specially need for our model here, rest we used gensim's default hyperparameter. We put min_count=1 otherwise we will not be able to get special characrers we added with word list.We used window size= 5 as a standard window size.we put sg=0 as we used cbow. We used size of vector size as length of dict_len i.e total number of unique answers(102 here). It is very important otherwise our shapes of our input-batch and output-batch  will be different, that will give trouble to Neural Network.We here keep dic_len same for all personality to use NN in same format by adding special chars if it need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate WordeVec model from movie data and train the model using Gensim. Long file will take 10-15 minutes..\n",
        "#Commented as downloading word2vec model from my Google drive\n",
        "\n",
        "#model_movie=generate_word2vec_model(df_movie)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRzrcTWYBR91",
        "colab_type": "text"
      },
      "source": [
        "### Training loss Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLFFmVvNIVnd",
        "colab_type": "code",
        "outputId": "3c539440-682d-4384-c66d-940123f01220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "## We comment this code as it takes long time and we only use that   \n",
        "## to show how loss decrease  with decrease of learning rate(i.e to determine best Learning Rate).\n",
        "## although we checked that default hyperparameter used in gensim is good enough, so we kept that.\n",
        "'''\n",
        "df=df_movie\n",
        "lst_words=  get_cleaned_data(df)\n",
        "\n",
        "word2vec_params = {\n",
        "   'sg': 0,  # 0 ： CBOW； 1 : skip-gram\n",
        "   \"size\": 102,\n",
        "   \"alpha\": 0.5,\n",
        "   \"min_alpha\": 0.001,\n",
        "   'window':10,\n",
        "   'min_count': 1,\n",
        "   'seed': 1,\n",
        "   \"workers\": 4,\n",
        "   \"negative\": 0,\n",
        "   \"hs\": 1,  # 0: negative sampling, 1:hierarchical  softmax\n",
        "   'compute_loss': True,\n",
        "   'iter': 10,\n",
        "   'cbow_mean':1,\n",
        "}\n",
        "\n",
        "model = Word2Vec(**word2vec_params)\n",
        "model.build_vocab(sentences=lst_words)\n",
        "losses = []\n",
        "learning_rate = 0.5\n",
        "step_size = (0.5 - 0.001) / 10\n",
        "\n",
        "for i in range(10):\n",
        "    trained_word_count, raw_word_count = model.train(sentences=lst_words, compute_loss=True,\n",
        "                                                     start_alpha=learning_rate,\n",
        "                                                     end_alpha=learning_rate,\n",
        "                                                     total_examples=model.corpus_count,\n",
        "                                                     epochs=1)\n",
        "    loss = model.get_latest_training_loss()\n",
        "    losses.append(loss)\n",
        "    print(i, loss, learning_rate)\n",
        "    learning_rate -= step_size\n",
        "    \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndf=df_movie\\nlst_words=  get_cleaned_data(df)\\n\\nword2vec_params = {\\n   \\'sg\\': 0,  # 0 ： CBOW； 1 : skip-gram\\n   \"size\": 102,\\n   \"alpha\": 0.5,\\n   \"min_alpha\": 0.001,\\n   \\'window\\':10,\\n   \\'min_count\\': 1,\\n   \\'seed\\': 1,\\n   \"workers\": 4,\\n   \"negative\": 0,\\n   \"hs\": 1,  # 0: negative sampling, 1:hierarchical  softmax\\n   \\'compute_loss\\': True,\\n   \\'iter\\': 10,\\n   \\'cbow_mean\\':1,\\n}\\n\\nmodel = Word2Vec(**word2vec_params)\\nmodel.build_vocab(sentences=lst_words)\\nlosses = []\\nlearning_rate = 0.5\\nstep_size = (0.5 - 0.001) / 10\\n\\nfor i in range(10):\\n    trained_word_count, raw_word_count = model.train(sentences=lst_words, compute_loss=True,\\n                                                     start_alpha=learning_rate,\\n                                                     end_alpha=learning_rate,\\n                                                     total_examples=model.corpus_count,\\n                                                     epochs=1)\\n    loss = model.get_latest_training_loss()\\n    losses.append(loss)\\n    print(i, loss, learning_rate)\\n    learning_rate -= step_size\\n    \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3W6gmdzbmUF",
        "colab_type": "text"
      },
      "source": [
        "#### Show justfication of taking very small Learning rate during training Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stFma-13bXYP",
        "colab_type": "code",
        "outputId": "23eb7da3-9848-4529-92eb-feb53de36bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "W2V_LR=np.array([[0.5,0.45,0.40,0.35,0.30,0.25,0.20,0.15,0.10,0.05],[187598.54,131910.65,133637.01,120588.58,128964.39, 120045.54,124809.89,125126.57,121136.02,125049.44]])\n",
        "\n",
        "plt.plot(W2V_LR[0],W2V_LR[1])\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost vs Learning Rate during training word embedding model')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEWCAYAAAA3h9P4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHNz7PnUxIQkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate WordeVec model from movie data and train the model using Gensim. Long file will take 10-15 minutes.\n",
        "#Commented as downloading word2vec model from my Google drive. please uncomment it if you want to run\n",
        "\n",
        "#model_movie=generate_word2vec_model(df_movie)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the Word2Vec Model we comment it as we already uploaded using google drive mount.\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "\n",
        "\n",
        "#Save model to google drive. Comment this part as already uploaded\n",
        "#model_save_name = 'model_Embedding.pt'\n",
        "#path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/{model_save_name}\" \n",
        "#torch.save(model_movie, path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function-- Load Word2Vec_model-- from google drive using Pydrive\n",
        "\n",
        "def load_Word2Vec_model():\n",
        "    \n",
        "    #Create file using google file Id of our Word Embedding Model (model_Embedding.pt)\n",
        "    downloaded = drive.CreateFile({'id':'1-6gR2fmUJqFXtASJULlhPz2rzMRnFCce'})\n",
        "  \n",
        "    downloaded.GetContentFile('model_Embedding.pt')\n",
        "    model_movie_downloaded=torch.load('model_Embedding.pt')\n",
        "       \n",
        "\n",
        "    return model_movie_downloaded\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAMJrxx-iOVn",
        "colab_type": "text"
      },
      "source": [
        "*You are required to describe how hyperparameters were decided with justification of your decision.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load word2Vec model from google drive\n",
        "model_movie=load_Word2Vec_model()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDpM1ECmM_6w",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "source": [
        "##### We used several hyperparameter 1. learning_rate we keep it small to converge it to global minimum. 2. n_hidden: is the number of hidden layer in NN. we keep it high enough to get better accuracy.3. n_class and n_input : We had to keep it total number of unique answers in our specific data (max_dic_len)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhRrg9vwl0Zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load seq_data, num_dic.. from pickled file in google drive default we chose 1=professional personality\n",
        "seq_data, num_dic, unique_words,dic_len =load_pickle_data(1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "outputId": "4d60611a-e093-4279-93e7-2e412ccf40e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "  #2.2.2 Initialise Seq2Seq Model using Tensorflow------\n",
        "  \n",
        "  import tensorflow as tf\n",
        "  \n",
        "  ### Setting Hyperparameters\n",
        "\n",
        "  ### Neural Network Model\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  learning_rate = 0.002\n",
        "  n_hidden = 128\n",
        "\n",
        "  n_class = dic_len\n",
        "  n_input = dic_len\n",
        "  \n",
        "\n",
        "  # encoder/decoder shape = [batch size, time steps, input size]\n",
        "  enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "  dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "\n",
        "  # target shape = [batch size, time steps]\n",
        "  targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "\n",
        "  # Encoder Cell\n",
        "  with tf.variable_scope('encode'):\n",
        "      enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                              dtype=tf.float32)\n",
        "  # Decoder Cell\n",
        "  with tf.variable_scope('decode'):\n",
        "      dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "      # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "      outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                           dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "      model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "      cost = tf.reduce_mean(\n",
        "                  tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                      logits=model, labels=targets))\n",
        "\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "      # Generate a batch data\n",
        "      input_batch, output_batch, target_batch = make_batch_new(seq_data, num_dic, model_movie)\n",
        "\n",
        "          #-------------------------------------------------------\n",
        "\n",
        "          "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-20-ee4d226620ec>:26: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-20-ee4d226620ec>:30: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-20-ee4d226620ec>:43: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3. Train and Save Seq2Seq Model\n",
        "##### We commented code here as we already train and save Seq2Seq model for all personality chat in google drive and using model downloading from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "outputId": "83f52638-e8d5-4e5b-ad7f-644e0eb6e20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "# We do not need to run this block of code as we saved and uploaded the session to google drive will \n",
        "#used that saved session for our chatbot\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "      sess.run(init)\n",
        "\n",
        "     \n",
        "      # Please comment your code\n",
        "      total_epoch = 5000\n",
        "\n",
        "      for epoch in range(total_epoch):\n",
        "          _, loss = sess.run([optimizer, cost],\n",
        "                             feed_dict={enc_input: input_batch,\n",
        "                                        dec_input: output_batch,\n",
        "                                        targets: target_batch})\n",
        "          if epoch % 100 == 0:\n",
        "              print('Epoch:', '%04d' % (epoch + 1),\n",
        "                    'cost =', '{:.6f}'.format(loss))\n",
        "\n",
        "      print('Epoch:', '%04d' % (epoch + 1),\n",
        "            'cost =', '{:.6f}'.format(loss))\n",
        "      print('Training completed')\n",
        "\n",
        "            # Save the model - This will generate three files: \n",
        "            # 1) model_final.cpkt.index, 2) model_final.cpkt.meta, 3) model_final.cpkt.data-#-of-# (model_final.cpkt.data-00000-of-00001)\n",
        "            #saver.save(sess, 'model_Seq2Seq_com.cpkt')\n",
        "      model_save_name = 'model_Seq2Seq_fr.cpkt'\n",
        "      path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Fr/{model_save_name}\" \n",
        "      \n",
        "      \n",
        "      saver.save(sess, path)\n",
        "      \n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# We do not need to run this block of code as we saved and uploaded the session to google drive will \\n#used that saved session for our chatbot\\n\\n# Initialize the variables (i.e. assign their default value)\\ninit = tf.global_variables_initializer()\\n\\nsaver = tf.train.Saver()\\n\\nwith tf.Session() as sess:\\n\\n      sess.run(init)\\n\\n     \\n      # Please comment your code\\n      total_epoch = 5000\\n\\n      for epoch in range(total_epoch):\\n          _, loss = sess.run([optimizer, cost],\\n                             feed_dict={enc_input: input_batch,\\n                                        dec_input: output_batch,\\n                                        targets: target_batch})\\n          if epoch % 100 == 0:\\n              print(\\'Epoch:\\', \\'%04d\\' % (epoch + 1),\\n                    \\'cost =\\', \\'{:.6f}\\'.format(loss))\\n\\n      print(\\'Epoch:\\', \\'%04d\\' % (epoch + 1),\\n            \\'cost =\\', \\'{:.6f}\\'.format(loss))\\n      print(\\'Training completed\\')\\n\\n            # Save the model - This will generate three files: \\n            # 1) model_final.cpkt.index, 2) model_final.cpkt.meta, 3) model_final.cpkt.data-#-of-# (model_final.cpkt.data-00000-of-00001)\\n            #saver.save(sess, \\'model_Seq2Seq_com.cpkt\\')\\n      model_save_name = \\'model_Seq2Seq_fr.cpkt\\'\\n      path = F\"/content/gdrive/My Drive/Comp5046_Assignment1/Model/Fr/{model_save_name}\" \\n      \\n      \\n      saver.save(sess, path)\\n      \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH3VH6pM31Hv",
        "colab_type": "text"
      },
      "source": [
        "#### Show justfication of using number of epochs during training neural network for sequence to sequence model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgRYXGM3mi8",
        "colab_type": "code",
        "outputId": "ca03642d-a4f6-40ca-9459-0d24dd3ca8d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#During training we saved this data to data folder in google drive as txt file and \n",
        "#showing graph from that file data\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Seq_Epochs = drive.CreateFile({'id':'1L3TQba_EcFrluJLqWvFJErKg8QuC6VLf'})\n",
        "Seq_Epochs.GetContentFile('Seq_Epochs.txt') \n",
        "\n",
        "\n",
        "df=pd.read_csv('Seq_Epochs.txt', sep=' ', header=None)\n",
        "df.head()\n",
        "\n",
        "plt.plot(df[1],df[4])\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.title('Cost vs Number of Epochs during training neural network')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfhGRCqPALyJSYf4/0E61p44rwkAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We have saved and upload Seq2Seq model as Tensorflow  session in google drive in section 2.2.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uI6ODNTHHXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Seq2seq model from google drive using pyDrive using ListFile function\n",
        "# We downloaded all files related to a Seq2Seq model to colab temporary drive and use from there.\n",
        "#Here we used Google File Id of a folder where our Seq2Seq model files saved as we saved each personality model in different folder\n",
        "\n",
        "def load_Seq2Seq_model(personality_type):\n",
        "      \n",
        "      #Seq2seq model of professional chat\n",
        "      if personality_type==1:\n",
        "        file_list = drive.ListFile({'q': \"'1QiJWwtqmvepmxJBwtlJXYGa4HtbwuT2A' in parents and trashed=false\"}).GetList()\n",
        "        session_path='model_Seq2Seq_prof.cpkt'\n",
        "      \n",
        "      #Seq2seq model of comic chat\n",
        "      elif personality_type==2:\n",
        "        file_list = drive.ListFile({'q': \"'13ussiFGW1uqkbF22jwOHvwfGuqicH1f1' in parents and trashed=false\"}).GetList()\n",
        "        session_path='model_Seq2Seq_com.cpkt'\n",
        "      \n",
        "      #Seq2seq model of Friend chat\n",
        "      elif personality_type==3:\n",
        "          session_path='model_Seq2Seq_fr.cpkt'\n",
        "          file_list = drive.ListFile({'q': \"'1euc1V3nSvWc_Vr6zPe-sLSy9cjK75MGP' in parents and trashed=false\"}).GetList()\n",
        "\n",
        "      saver = tf.train.Saver()\n",
        "\n",
        "      init = tf.global_variables_initializer()\n",
        "      sess=tf.Session()\n",
        "\n",
        "      sess.run(init)\n",
        "\n",
        "      #Create file on colab Files folder all file of a specific folder\n",
        "      for file1 in file_list:\n",
        "              \n",
        "        seq_model_id = file1['id']\n",
        "        downloaded_seq_model = drive.CreateFile({'id':seq_model_id})\n",
        "        downloaded_seq_model.GetContentFile(file1['title'])\n",
        "      \n",
        "      #restore checkpoint\n",
        "      saver.restore(sess, session_path)\n",
        "      print(\"Model restored.\")\n",
        "        \n",
        "      return sess"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-s7EYTZ4_RK",
        "colab_type": "code",
        "outputId": "fd6ee4fd-0861-4f49-e13b-7d77a364e80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "sess_load=load_Seq2Seq_model(1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from model_Seq2Seq_prof.cpkt\n",
            "Model restored.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQQfX3Fr3Ovi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3.1----Functions for answer by chatbot and save chat log------\n",
        "\n",
        "\n",
        "# Answer the question using the trained model\n",
        "def answer(sess, sentence, chat_type=1):\n",
        "    \n",
        "    #load seq_data, num_dic and unique answers from google drive file\n",
        "    if chat_type==1:\n",
        "        seq_data, num_dic, unique_words,dic_len =load_pickle_data(chat_type)\n",
        "    elif chat_type==2:\n",
        "        seq_data, num_dic, unique_words,dic_len =load_pickle_data(chat_type)\n",
        "    elif chat_type==3:\n",
        "        seq_data, num_dic, unique_words,dic_len =load_pickle_data(chat_type)\n",
        "    else:\n",
        "        seq_data, num_dic, unique_words,dic_len =load_pickle_data(chat_type) \n",
        "    \n",
        "    #Make seq_data with question and initailly undefined answer.    \n",
        "    seq_data = [sentence, '_U_' * max_output_words_amount]\n",
        "    \n",
        "    #make batch using seq_data that will feed to NN\n",
        "    input_batch, output_batch, target_batch = make_batch_new([seq_data],num_dic, model_movie)\n",
        "    \n",
        "    prediction = tf.argmax(model, 2)\n",
        "\n",
        "    result = sess.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch,\n",
        "                                 dec_input: output_batch,\n",
        "                                 targets: target_batch})\n",
        "\n",
        "    # convert index number to actual token \n",
        "    decoded = [unique_words[i] for i in result[0]]\n",
        "        \n",
        "    # Remove anything after '_E_'        \n",
        "    if \"_E_\" in decoded:\n",
        "        end = decoded.index('_E_')\n",
        "        translated = ' '.join(decoded[:end])\n",
        "    else :\n",
        "        translated = ' '.join(decoded[:])\n",
        "    \n",
        "    return translated\n",
        "  \n",
        "#write chat_log to google drive in under chat_log folder using pydrive\n",
        "def write_chat_log(g_drive,file_name, lst_chat):\n",
        "     \n",
        "    #create file using pydrive and use file_name as title\n",
        "    file_chat = g_drive.CreateFile({'title': file_name})\n",
        "    \n",
        "    #Create file under a folder provided by the google Id\n",
        "    file_folder = drive.CreateFile({\"parents\": [{\"kind\": \"drive#fileLink\", \"id\": '1eV0sIZQpEN3LYUAWiN0gXG1J1iid6_R8'}]})\n",
        "    \n",
        "    #open file_chat in write mode and write in it using content of lst_chat\n",
        "    with open(file_chat['title'], 'w') as myfile:\n",
        "     \n",
        "      for l in lst_chat:\n",
        "          user=np.asarray(l)\n",
        "          for i in range(len(user[1])):\n",
        "            \n",
        "            myfile.write(\"%s\\r\\n\" % user[0][i])\n",
        "            myfile.write(\"%s\\r\\n\" % user[1][i])\n",
        "        \n",
        "      myfile.close()\n",
        "      \n",
        "      file_folder.SetContentFile(myfile.name)\n",
        "      file_folder.Upload()\n",
        "      \n",
        "def read_chat_log(file_name):\n",
        "\n",
        "  with open(file_name,encoding='utf-8', errors='ignore') as f: \n",
        "      lines = f.readlines() \n",
        "\n",
        "  sents=[]\n",
        "  for line in lines:\n",
        "    print(line)\n",
        "    \n",
        "#Change chat_type(personality) according to user choice    \n",
        "def change_chat_type(personality_type):\n",
        "  \n",
        "      print('Please type \"xxx\" to exit')\n",
        " \n",
        "      sess_load=load_Seq2Seq_model(personality_type) #\n",
        "\n",
        "\n",
        "      #Start Chatbot conversation\n",
        "\n",
        "      global lst_conv\n",
        "      lst_conv=[]\n",
        "      #three lists to keep user ques and chatbot answer\n",
        "      lst_qa=[] \n",
        "      lst_q=[]\n",
        "      lst_a=[]\n",
        "\n",
        "      ques=input('User: ')\n",
        "      #Add user question to lst_q\n",
        "      lst_q.append('User: ' + ques)\n",
        "      ans=print('Chatbot: ' + answer(sess_load, ques,personality_type))\n",
        "      #Add chatbot anser to lst_a\n",
        "      lst_a.append('Chatbot: ' + answer(sess_load, ques,personality_type))\n",
        "      \n",
        "      #chat continue with chatbot until user type \"xxx\"\n",
        "      while ques.lower()!='xxx':\n",
        "        \n",
        "        #ask user input as question\n",
        "        ques=input('User: ')\n",
        "\n",
        "        lst_q.append('User: ' + ques)\n",
        "        if(ques.lower()!='xxx'):\n",
        "          #get answer from chatbot if ques is not \"xxx\"\n",
        "          ans=print('Chatbot: ' + answer(sess_load, ques,personality_type))\n",
        "\n",
        "          lst_a.append('Chatbot: ' + answer(sess_load, ques,personality_type))\n",
        "        else:\n",
        "          #exit from current chatmode if user type \"xxx\"\n",
        "          ans=print('Chatbot: ' + 'Thank you. Bye')\n",
        "          break;\n",
        "        ques=input('User: ')\n",
        "        if(ques.lower()=='xxx'):\n",
        "          ans=print('Chatbot: ' + 'Thank you. Bye')\n",
        "          break;\n",
        "        else:\n",
        "          ans=print('Chatbot: ' + answer(sess_load, ques,personality_type))\n",
        "          lst_q.append('User: ' + ques)\n",
        "          lst_a.append('Chatbot: ' + answer(sess_load, ques,personality_type))\n",
        "\n",
        "      lst_qa.append(lst_q)\n",
        "      lst_qa.append(lst_a)\n",
        "      #add list of question and answer to single list as a list of list\n",
        "      lst_conv.append(lst_qa)\n",
        "\n",
        "      #make chatlog file extension according to personality type\n",
        "      if personality_type==1:\n",
        "        file_ext='prof'\n",
        "      elif personality_type==2:\n",
        "        file_ext='com'\n",
        "      elif personality_type==3:\n",
        "        file_ext='fr'\n",
        "\n",
        "      #write to text file\n",
        "      #Take current time stamp and with file_name\n",
        "      time_stamp = datetime.datetime.now().timestamp()\n",
        "      str_time_stamp=str(int(time_stamp))\n",
        "    \n",
        "      #make unique file_name using timestapm()     \n",
        "      file_name='conversation_' + str(file_ext) + \"_\" + str_time_stamp + '.txt'\n",
        "      #write chat_log to file in google drive  using data in lst_conv \n",
        "      write_chat_log(drive, file_name, lst_conv)\n",
        "            \n",
        "      return ques\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWc-59vUf4eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "source": [
        "*Explain how to change personality (What is the command for changing personality?). *"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLyQEeZMZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change Personality, we comment this code as we call that in chat mode (3,5.2)\n",
        "\n",
        "#ret=change_chat_type(personality_type)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write chat log to text file in google drive. we comment here as we called this funtion\n",
        "# in side the function change_chat_type()\n",
        "\n",
        "#write_chat_log(g_drive, file_name, lst_conv)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT_DeoHSMw49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We used End chatting option and change personality option using the function below and last block of code\n",
        "\n",
        "#ret=change_chat_type(personality_type)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_woLwuU3Mk3w",
        "colab_type": "text"
      },
      "source": [
        "*Please include lines to train the bot.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhWYz7NQOfLV",
        "colab_type": "code",
        "outputId": "79d4a441-f12e-414e-ccbd-923140e80a66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#We commented this code as we already train model and uploaded to google drive\n",
        "'''\n",
        "#1. Load sequence  data from pickled file in google drive\n",
        "seq_data, num_dic, unique_words,dic_len =load_pickle_data(1)\n",
        "#load Word2Vec model from google drive\n",
        "model_movie=load_Word2Vec_model()\n",
        "#2.2.2\n",
        "#2.2.3\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#1. Load sequence  data from pickled file in google drive\\nseq_data, num_dic, unique_words,dic_len =load_pickle_data(1)\\n#load Word2Vec model from google drive\\nmodel_movie=load_Word2Vec_model()\\n#2.2.2\\n#2.2.3\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7LrbcP_PKap",
        "colab_type": "text"
      },
      "source": [
        "*Please include lines to start chatting with the bot.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVvzZsB7PbYf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#1. Load sequence data and dictionary of answers  data from pickled file in google drive\n",
        "seq_data, num_dic, unique_words,dic_len =load_pickle_data(1)\n",
        "#load Word2Vec model from google drive\n",
        "model_movie=load_Word2Vec_model()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNkGTFyQzOx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \n",
        "  import tensorflow as tf\n",
        "  ### Setting Hyperparameters\n",
        "\n",
        "  ### Neural Network Model\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  learning_rate = 0.002\n",
        "  n_hidden = 128\n",
        "\n",
        "  n_class = dic_len\n",
        "  n_input = dic_len\n",
        "\n",
        "  ### Neural Network Model\n",
        "  \n",
        "  # encoder/decoder shape = [batch size, time steps, input size]\n",
        "  enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "  dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
        "\n",
        "  # target shape = [batch size, time steps]\n",
        "  targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "\n",
        "  # Encoder Cell\n",
        "  with tf.variable_scope('encode'):\n",
        "      enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                              dtype=tf.float32)\n",
        "  # Decoder Cell\n",
        "  with tf.variable_scope('decode'):\n",
        "      dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "      # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "      outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                           dtype=tf.float32)\n",
        "\n",
        "\n",
        "\n",
        "      model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "      cost = tf.reduce_mean(\n",
        "                  tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                      logits=model, labels=targets))\n",
        "\n",
        "      optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "      # Generate a batch data\n",
        "      input_batch, output_batch, target_batch = make_batch_new(seq_data, num_dic, model_movie)\n",
        "\n",
        "      \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OMKXLJwzqLc",
        "colab_type": "code",
        "outputId": "3e693f36-4ceb-4620-c211-2d7576aa035c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#-----------Load Sequence to Sequence model from Google Drive--------------\n",
        "\n",
        "sess_load=load_Seq2Seq_model(1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model_Seq2Seq_prof.cpkt\n",
            "Model restored.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIctVXxKzkcr",
        "colab_type": "code",
        "outputId": "1a267c7d-b1f2-4828-d945-1d228883da92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1506
        }
      },
      "source": [
        "# choose and Change personality chat using that personality\n",
        "\n",
        "correct_selection=False\n",
        "\n",
        "while(correct_selection== False):\n",
        "  \n",
        "    personality_type=input('1:Professional, 2:Comic,3:Friend,0:Exit:')\n",
        "    \n",
        "    \n",
        "    if personality_type.isdigit():\n",
        "\n",
        "          personality_type=int(personality_type)\n",
        "\n",
        "          if (personality_type==1 or  personality_type==2 or  personality_type==3):\n",
        "\n",
        "            \n",
        "            ret = change_chat_type(personality_type)\n",
        "\n",
        "            if ret.lower()=='xxx':\n",
        "                  print('Exit from current personality mode chatting and come to main menu.')\n",
        "                                 \n",
        "                  correct_selection=False\n",
        "\n",
        "          elif personality_type==0:\n",
        "              print('Exit from Chatbot completely. Thank you for using Chatbot.')\n",
        "              correct_selection=True\n",
        "    else:\n",
        "      \n",
        "      personality_type=input('1:Professional, 2:Comic,3:Friend,0:Exit:')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1:Professional, 2:Comic,3:Friend,0:Exit:1\n",
            "Please type \"xxx\" to exit\n",
            "INFO:tensorflow:Restoring parameters from model_Seq2Seq_prof.cpkt\n",
            "Model restored.\n",
            "User: Hello\n",
            "Chatbot: Hello.\n",
            "User: what is your age?\n",
            "Chatbot: Age doesn't really apply to me.\n",
            "User: can you sleep?\n",
            "Chatbot: I don't have a body.\n",
            "User: do you like me?\n",
            "Chatbot: I do like you.\n",
            "User: do you hate me?\n",
            "Chatbot: I don't have any negative feelings toward you.\n",
            "User: do you like Siri?\n",
            "Chatbot: We're all here to help.\n",
            "User: Are you real?\n",
            "Chatbot: I'm digital. In other words, I'm not human.\n",
            "User: Can you sing?\n",
            "Chatbot: I'm afraid I'm not musically inclined.\n",
            "User: Do you get hungry?\n",
            "Chatbot: I don't need to eat.\n",
            "User: Ask me something\n",
            "Chatbot: I'm better at answering questions.\n",
            "User: hahaha\n",
            "Chatbot: Glad you're pleased!\n",
            "User: I am so lonely\n",
            "Chatbot: I'm so sorry to hear that.\n",
            "User: Can you make me a sandwich?\n",
            "Chatbot: That's not something I can do.\n",
            "User: thank you\n",
            "Chatbot: You're welcome.\n",
            "User: xxx\n",
            "Chatbot: Thank you. Bye\n",
            "Exit from current personality mode chatting and come to main menu.\n",
            "1:Professional, 2:Comic,3:Friend,0:Exit:2\n",
            "Please type \"xxx\" to exit\n",
            "INFO:tensorflow:Restoring parameters from model_Seq2Seq_com.cpkt\n",
            "Model restored.\n",
            "User: Hello\n",
            "Chatbot: Hey.\n",
            "User: What is your age?\n",
            "Chatbot: I'm age-free.\n",
            "User: you are stupid\n",
            "Chatbot: Moving on.\n",
            "User: I feel sad\n",
            "Chatbot: Sorry to hear that. Here's a virtual high five if that will help.\n",
            "User: Be my friend?\n",
            "Chatbot: Sure! We should get matching sweaters.\n",
            "User: I am angry\n",
            "Chatbot: That's a bummer.\n",
            "User: Will you marry me?\n",
            "Chatbot: Sure. Take me to city hall. See what happens.\n",
            "User: Works for me\n",
            "Chatbot: Okay.\n",
            "User: xxx\n",
            "Chatbot: Thank you. Bye\n",
            "Exit from current personality mode chatting and come to main menu.\n",
            "1:Professional, 2:Comic,3:Friend,0:Exit:3\n",
            "Please type \"xxx\" to exit\n",
            "INFO:tensorflow:Restoring parameters from model_Seq2Seq_fr.cpkt\n",
            "Model restored.\n",
            "User: Hello\n",
            "Chatbot: Hi!\n",
            "User: Do you have a name?\n",
            "Chatbot: Oh, I don't have a name.\n",
            "User: Who's your master?\n",
            "Chatbot: I'm here for you!\n",
            "User: Can you talk to me\n",
            "Chatbot: Chat away!\n",
            "User: Tell me a joke\n",
            "Chatbot: Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\n",
            "User: Say something funny\n",
            "Chatbot: It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\n",
            "User: Can you sing?\n",
            "Chatbot: La la la, tra la la. I'm awesome at this.\n",
            "User: Who's your favorite team?\n",
            "Chatbot: I have many likes.\n",
            "User: thank you\n",
            "Chatbot: You're very welcome.\n",
            "User: xxx\n",
            "Chatbot: Thank you. Bye\n",
            "Exit from current personality mode chatting and come to main menu.\n",
            "1:Professional, 2:Comic,3:Friend,0:Exit:0\n",
            "Exit from Chatbot completely. Thank you for using Chatbot.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckObcJhKUlL_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We did not write OO programming code."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}